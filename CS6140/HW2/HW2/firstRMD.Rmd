---
title: "CS6140 Assignment 2"
author: "Chengbo Gu"
date: "2017-2-5 16:23:51"
---
```{r include=FALSE}
#source('Variables.R')
library(pastecs)
library(leaps)
library(glmnet)
#options(scipen=100)
options(digits=4)
```
## Problem 1
## Problem 2
How to classify Alzheimer¡¯s disease and identify those presymptomatic individuals with mild cognitive impairment (MCI) who will eventually convert to Alzheimer¡¯s is the issue addressed in Sandip Ray¡¯s manuscript. Statistical methods including significance analysis of microarrays (SAM), unsupervised clustering algorithm, predictive analysis of microarrays (PAM, 10-fold cross-validation) were used in Ray¡¯s work. His conclusion is that 18 signaling proteins in blood plasma are found that can be used to classify blinded samples from Alzheimer¡¯s and control subjects with close to 90% accuracy. Also, these signaling proteins can help identify patients who had MCI that progressed to Alzheimer¡¯s disease 2-6 years later. Biological analysis of the 18 proteins points to systemic dysregulation of hematopoiesis, immune responses, apoptosis and neuronal support in presymptomatic Alzheimer¡¯s disease, which makes the conclusion more convincing.  


The first and most interesting part of Ray¡¯s work for me is that whenever he draws a conclusion, he wants to verify it from other perspectives. For instance, after SAM identified 19 proteins with highly significant differences in expression between Alzheimer¡¯s and NDC samples, he applied an unsupervised clustering algorithm based on the similarity in abundance of these 19 markers which produced two main clusters that contained mostly Alzheimer¡¯s or NDC samples respectively. Similarly, after PAM identified 18 predictors and classified Alzheimer¡¯s and NDC samples with very high accuracy, the unsupervised clustering based on these 18 markers was able to separate Alzheimer¡¯s and NDC samples at the same time. From high level, this manuscript is not merely about statistical analysis but combined with the biological one. Moreover, they are consistent with each other.


Another impressive part is that these 18 predictors also perform well in distinguishing Alzheimer¡¯s samples from other neurological diseases and rheumatoid arthritis. Though this may not be Ray¡¯s initial intention, the result does support his statement that these 18 biomarkers certainly form an Alzheimer¡¯s-specific signature.

To go further, we may want to find the reason why there exists a discrepancy between SAM and PAM ¡ª¡ª the missing biomarker CCL22. Why it shows significant difference but is discarded by PAM? What will the regression model and MSE be with 19 predictors? What¡¯s the correlation between CCL22 and M-CSF? CCL22 and M-CSF are the nearest in the phylogenetic tree, maybe CCL22 is somewhat collinear with M-CSF. By solving these problems, we may be able to explain the missing CCL22 thus makes the work more persuasive.

## Problem 3
To make predictions regarding post-college earnings and debt of alumni by machine learning models is the issue discussed in Monica¡¯s project. Techniques like single regression imputation, sequential forward-based feature selection, linear/locally weighted linear/K-nearest neighbors/support vector regression and neural networks were applied during the process. The incremental model selection process indicated that regression imputation of privacy-suppressed values improved performance of models. Besides, weighted linear regression outperformed other models with below 10% and 18% average error for earning and debt data respectively. Moreover, this model performed well towards law schools though law schools made up only little of the data.

One of the most attractive highlights in Monica¡¯s project is privacy-suppressed values handling. Instead of simply removing all features with any privacy-suppressed entries or merely setting missing values to the mean of observed values, single regression imputation was applied. To make the regression model statistically meaningful, they imposed a requirement that imputed features must have missing data for less than 30% of schools. Since privacy-suppressed values occurred in potentially useful metrics, the aforementioned implementation does increase the credibility of the model.

Model competing is a fascinating part in this project as well. There are totally 5 classes of optimized models among which best weighted linear regression model outperformed others. The authors discussed the superiority of the best weighted linear regression by exploring whether the model generalized well outside their dataset and got a positive result. Besides, they also analyzed the reason why support vector regression did much worse than all other models. Due to the reasonable steps that they gone through model refining, one could safely draw the conclusion that the best model among the 5 was indeed the best weighted linear regression.

Although there is no doubt that this project is amazing with huge workload, there are still some deficiencies as far as I am concerned. The very first one is the representativeness of the dataset the authors selected. Their goal was to make prediction of post-collegiate earnings and debt while the data they collected was from post-college earnings and debt of alumni who were on federal financial aid. Students who received federal financial aid may share some unique features that potentially influenced their earnings and debt. Without eliminating this possibility, the authors couldn¡¯t simply assume that their data is representative. I do know the fact that there are some cases when it is extreme hard to retrieve data, but I strongly recommend obtaining data from all kinds of alumni.

The second one could be the poor performance of their best model given data of trade schools. As the author stated, the current algorithm has trouble extending to such schools, which indicates that future earnings and debt might be best characterized by a different set of features. 

For future work, partnering with the U.S. Department of Education to gain access to all kinds of data (from all kinds of alumni and maybe some privacy-suppressed data) would be a big plus. More generalized models for imputation can be involved to handle the remaining privacy-suppressed data better if given enough computational resource. Finally, improving the performance towards trade schools by possibly selecting new features could form the project an organic whole.

## Problem 4
### (a) Select the training set

First, we read the data, retrieve trainSet and testSet.
Also build test model which will be used to calculate MSE later.
```{r}
mydata <- as.data.frame(read.table("prostate.txt"))
trainSet <- mydata[mydata$train == "TRUE", -c(10)]
testSet <- mydata[mydata$train == "FALSE", -c(10)]
x.test <- model.matrix(lpsa ~ ., data = testSet)
y.test <- testSet$lpsa
```

### (b) Data exploration
In this section, we will first identify outliers from training set and then perform one-variable
and two-variable summary statistics.
```{r plot, fig.width = 5, fig.height=4}
sum(is.na.data.frame(mydata))
# identify outliers
m.dist <- sort(mahalanobis(trainSet, colMeans(trainSet), cov(trainSet)), decreasing=TRUE)
plot(m.dist, ylab="manalanobis distance", xlab="index of m.dist", main = "Outliers Identification")
#m.dist.order <- order(mahalanobis(trainSet, colMeans(trainSet), cov(trainSet)), decreasing=TRUE)
#is.outlier   <- rep(FALSE, nrow(trainSet))
#is.outlier[m.dist.order[1:8]] <- TRUE

#trainSet_Outlier <- cbind(trainSet, is.outlier)
#trainSet <- trainSet_Outlier[trainSet_Outlier$is.outlier == FALSE, -c(10)]
```
As we can see from the plot above that the first 8 patients are seems to be outliers, thus we remove them from our training set. After this, we can apply one-variable and two-variable summary.

#### one variable summary

simple one variable summary
```{r}
# one variable summary

#options(digits=2)
summary(trainSet)
```

Of course we can explore some more details with following commands
```{r}
stat.desc(trainSet, basic=F)
stat.desc(trainSet, desc=F)
```
#### two variable summary
```{r}
# two variable summary
round(cor(trainSet), digits=2)
pairs(trainSet)
```

#### Discussion
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX




### (c) Assumption of Normality
```{r}
# Assumption of Normality
fit <- lm(lpsa ~ ., data=trainSet)
y <- fit$residuals
fit.summary <- summary(fit)
qqnorm(y, ylim=c(-2.5,2.5))
abline(a=0,b=1,col="red")
```
     
     
     
### (d) Variable selection and Performance evaluation
```{r}
#options(digits=4)
# variable selection 
# all subset
regfit.full <- regsubsets(lpsa ~ ., data=trainSet)
reg.summary <- summary(regfit.full)
reg.summary

par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS", type='l')
plot(reg.summary$adjr2, xlab="Number of variables", ylab="adjR2", type='l')
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type='l')
abline(a=0, b=1, lty=3, lwd=2)
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC", type='l')


coefi <- coef(regfit.full, which.min(reg.summary$bic))
coefi
y.pred.full <- x.test[, names(coefi)] %*% coefi


# lasso
grid=10^seq(10, -2, length=100)
lasso.mod <- glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1, lambda=grid)
windows()

plot(lasso.mod)
legend('topleft', legend = names(trainSet[,-c(9)]), col=1:8, lty=1)

cv.out <- cv.glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1)
#x.test.lasso
#y.lasso.pred <- predict(lasso.mod, as.matrix(x.test.lasso))
#y.lasso.pred
#MSE.lasso.test <- apply( (y.lasso.pred - y.test)^2, 2 ,mean )
plot(cv.out)
#points(log(lasso.mod$lambda), MSE.lasso.test, col = "blue", pch = 18)

cv.out$lambda.min
coef(cv.out, s="lambda.min")

y.pred.lasso <- predict(cv.out, newx=as.matrix(testSet[,-c(9)]), s="lambda.min")

# performance evaluation
# full subset
mean((y.pred.full - y.test)^2)
# lasso
mean((y.pred.lasso - y.test)^2)
```

