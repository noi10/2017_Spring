lasso.mod <- glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1, lambda=grid)
windows()
plot(lasso.mod)
legend('topleft', legend = names(trainSet[,-c(9)]), col=1:8, lty=1)
cv.out <- cv.glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1)
#x.test.lasso
#y.lasso.pred <- predict(lasso.mod, as.matrix(x.test.lasso))
#y.lasso.pred
#MSE.lasso.test <- apply( (y.lasso.pred - y.test)^2, 2 ,mean )
plot(cv.out)
#points(log(lasso.mod$lambda), MSE.lasso.test, col = "blue", pch = 18)
cv.out$lambda.min
coef(cv.out, s="lambda.min")
y.pred.lasso <- predict(cv.out, newx=as.matrix(testSet[,-c(9)]), s="lambda.min")
# performance evaluation
# full subset
mean((y.pred.full - y.test)^2)
# lasso
mean((y.pred.lasso - y.test)^2)
#options(digits=4)
# variable selection
# all subset
regfit.full <- regsubsets(lpsa ~ ., data=trainSet)
reg.summary <- summary(regfit.full)
reg.summary
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS", type='l')
plot(reg.summary$adjr2, xlab="Number of variables", ylab="adjR2", type='l')
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type='l')
abline(a=0, b=1, lty=3, lwd=2)
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC", type='l')
coefi <- coef(regfit.full, which.min(reg.summary$bic))
coefi
y.pred.full <- x.test[, names(coefi)] %*% coefi
x.nonlinear.train <- cbind(trainSet$lpsa, trainSet$lcavol, trainSet$lweight, trainSet$lcavol*trainSet$lweight)
colnames(x.nonlinear.train)<- c("lpsa", "lcavol", "lweight", "lcavol*lweight")
x.nonlinear.test <- cbind(testSet$lpsa, testSet$lcavol, testSet$lweight, testSet$lcavol*testSet$lweight)
colnames(x.nonlinear.test)<- c("lpsa", "lcavol", "lweight", "lcavol*lweight")
x.nonlinear.test <- model.matrix(lpsa ~ ., data = as.data.frame(x.nonlinear.test))
nonlinear.fit <- lm(lpsa ~ ., data=as.data.frame(x.nonlinear.train))
coefi <- coef(nonlinear.fit)
y.nonlinear.pred.full <- x.nonlinear.test[, names(coefi)] %*% coefi
# lasso
grid=10^seq(10, -2, length=100)
lasso.mod <- glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1, lambda=grid)
windows()
plot(lasso.mod)
legend('topleft', legend = names(trainSet[,-c(9)]), col=1:8, lty=1)
cv.out <- cv.glmnet(x=as.matrix(trainSet[,-c(9)]), y=trainSet[,9], alpha=1)
#x.test.lasso
#y.lasso.pred <- predict(lasso.mod, as.matrix(x.test.lasso))
#y.lasso.pred
#MSE.lasso.test <- apply( (y.lasso.pred - y.test)^2, 2 ,mean )
plot(cv.out)
#points(log(lasso.mod$lambda), MSE.lasso.test, col = "blue", pch = 18)
cv.out$lambda.min
coef(cv.out, s="lambda.min")
y.pred.lasso <- predict(cv.out, newx=as.matrix(testSet[,-c(9)]), s="lambda.min")
# performance evaluation
# full subset
mean((y.pred.full - y.test)^2)
# lasso
mean((y.pred.lasso - y.test)^2)
View(y.nonlinear.pred.full)
View(trainSet)
View(trainSet)
View(trainSet)
View(testSet)
View(testSet)
#options(digits=4)
# variable selection
# all subset
regfit.full <- regsubsets(lpsa ~ ., data=trainSet)
reg.summary <- summary(regfit.full)
reg.summary
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of variables", ylab="RSS", type='l')
plot(reg.summary$adjr2, xlab="Number of variables", ylab="adjR2", type='l')
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp", type='l')
abline(a=0, b=1, lty=3, lwd=2)
plot(reg.summary$bic, xlab="Number of variables", ylab="BIC", type='l')
coefi <- coef(regfit.full, which.min(reg.summary$bic))
y.pred.full <- x.test[, names(coefi)] %*% coefi
coefi
x.interaction.train <- cbind(trainSet[,1], trainSet[,2], trainSet[,9],
trainSet[,1]^2, trainSet[,2]^2, trainSet$lcavol*trainSet$lweight)
colnames(x.interaction.train)<- c( "lcavol", "lweight", "lpsa", "lcavol^2", "lweight^2", "lcavol*lweight")
x.interaction.test <- cbind(testSet[,1], testSet[,2], testSet[,9],
testSet[,1]^2, testSet[,2]^2, testSet$lcavol*testSet$lweight)
colnames(x.interaction.test)<- c( "lcavol", "lweight", "lpsa", "lcavol^2", "lweight^2", "lcavol*lweight")
x.interaction.test <- model.matrix(lpsa ~ ., data = as.data.frame(x.interaction.test[,-c(4:5)]))
interaction.fit <- lm(lpsa ~ ., data=as.data.frame(x.interaction.train[, -c(4:5)]))
coefi <- coef(interaction.fit)
y.interaction.pred <- x.interaction.test[, names(coefi)] %*% coefi
mean((y.interaction.pred - y.test)^2)
x.interaction.test <- model.matrix(lpsa ~ ., data = as.data.frame(x.interaction.test[,-c(6)]))
View(x.interaction.test)
View(x.interaction.test)
View(x.interaction.test)
View(x.interaction.test)
View(x.interaction.train)
obs <- c(1.636, 0.374, 0.534, 3.015, 0.932, 0.179)
obs <- c(1.636, 0.374, 0.534, 3.015, 0.932, 0.179)
# problem 1
# ln(L) = nln(theta) - n(theta)*(mean of X)
# qiu dao shu
# 1/theta - mean of X = 0
# theta = 1/ mean of X
obs <- c(1.636, 0.374, 0.534, 3.015, 0.932, 0.179)
nloglik <- function(theta) {
-sum(log(dexp(obs, rate = theta)))
}
optim(par = 1, nloglik)
1/mean(obs)
# problem 2
# (a) m = sample mean = 100.8
100.8+qt(0.1, df=52)*(12.4/sqrt(53))
# 98.5891 ~ Inf
# problem 3
data(golub, package = 'multtest')
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
Zyxin.ALL <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "ALL"]
Zyxin.AML <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "AML"]
length.ALL <- length(Zyxin.ALL)
length.AML <- length(Zyxin.AML)
# (a) and (c)
nboot <- 1000
boot.mean.ALL <- rep(NA, nboot)
boot.mean.AML <- rep(NA, nboot)
boot.var.ALL <- rep(NA, nboot)
boot.var.AML <- rep(NA, nboot)
boot.median.ALL <- rep(NA, nboot)
boot.median.AML <- rep(NA, nboot)
for (i in 1:nboot) {
data.ALL.star <- Zyxin.ALL[sample(1:length.ALL, replace=TRUE)]
data.AML.star <- Zyxin.AML[sample(1:length.AML, replace=TRUE)]
boot.mean.ALL[i] <- mean(data.ALL.star)
boot.mean.AML[i] <- mean(data.AML.star)
boot.var.ALL[i] <- var(data.ALL.star)
boot.var.AML[i] <- var(data.AML.star)
boot.median.ALL[i] <- median(data.ALL.star)
boot.median.AML[i] <- median(data.AML.star)
}
quantile(boot.mean.ALL,c(0.025,0.975))
quantile(boot.mean.AML,c(0.025,0.975))
quantile(boot.var.ALL, c(0.025,0.975))
quantile(boot.var.AML, c(0.025,0.975))
quantile(boot.median.ALL,c(0.025,0.975))
quantile(boot.median.AML,c(0.025,0.975))
# (b)
# mean
ci.mean.ALL <- mean(Zyxin.ALL) + qt(c(0.025,0.975), df=length.ALL-1)*sd(Zyxin.ALL)/sqrt(length.ALL)
ci.mean.AML <- mean(Zyxin.AML) + qt(c(0.025,0.975), df=length.AML-1)*sd(Zyxin.AML)/sqrt(length.AML)
# variance
ci.variance.ALL <- c (var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.975, df=length.ALL-1),
var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.025, df=length.ALL-1))
ci.variance.AML <- c (var(Zyxin.AML)*(length.AML-1)/ qchisq(0.975, df=length.AML-1),
var(Zyxin.AML)*(length.AML-1)/ qchisq(0.025, df=length.AML-1))
# lecture page 39
# problem 4
MCsim <- function(nsim, lamb){
dataset <- matrix(rpois(nsim*50, lambda = lamb), nrow = nsim)
t0.05 <- qt(0.05, 49)
t0.95 <- -t0.05
kai0.95 <- qchisq(0.95, 49)
kai0.05 <- qchisq(0.05, 49)
means <- apply(dataset, 1, mean)
vars <- apply(dataset, 1, var)
interval.method1 <- matrix(data=NA, nrow=nsim, ncol=2)
interval.method2 <- matrix(data=NA, nrow=nsim, ncol=2)
counter1 <- 0
counter2 <- 0
for (i in 1:nsim) {
interval.method1[i,] <- means[i] + c( t0.05*sqrt(means[i]/50) , t0.95*sqrt(means[i]/50))
interval.method2[i,] <- 49*vars[i]/ ( c( kai0.95, kai0.05) )
if (interval.method1[i,1] <= lamb && interval.method1[i,2] >= lamb) {
counter1 <- counter1 + 1
}
if (interval.method2[i,1] <= lamb && interval.method2[i,2] >= lamb) {
counter2 <- counter2 + 1
}
}
print(counter1/nsim)
print(counter2/nsim)
}
# problem 1
# ln(L) = nln(theta) - n(theta)*(mean of X)
# qiu dao shu
# 1/theta - mean of X = 0
# theta = 1/ mean of X
obs <- c(1.636, 0.374, 0.534, 3.015, 0.932, 0.179)
nloglik <- function(theta) {
-sum(log(dexp(obs, rate = theta)))
}
optim(par = 1, nloglik)
1/mean(obs)
# problem 2
# (a) m = sample mean = 100.8
100.8+qt(0.1, df=52)*(12.4/sqrt(53))
# 98.5891 ~ Inf
# problem 3
data(golub, package = 'multtest')
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
Zyxin.ALL <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "ALL"]
Zyxin.AML <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "AML"]
length.ALL <- length(Zyxin.ALL)
length.AML <- length(Zyxin.AML)
# (a) and (c)
nboot <- 1000
boot.mean.ALL <- rep(NA, nboot)
boot.mean.AML <- rep(NA, nboot)
boot.var.ALL <- rep(NA, nboot)
boot.var.AML <- rep(NA, nboot)
boot.median.ALL <- rep(NA, nboot)
boot.median.AML <- rep(NA, nboot)
for (i in 1:nboot) {
data.ALL.star <- Zyxin.ALL[sample(1:length.ALL, replace=TRUE)]
data.AML.star <- Zyxin.AML[sample(1:length.AML, replace=TRUE)]
boot.mean.ALL[i] <- mean(data.ALL.star)
boot.mean.AML[i] <- mean(data.AML.star)
boot.var.ALL[i] <- var(data.ALL.star)
boot.var.AML[i] <- var(data.AML.star)
boot.median.ALL[i] <- median(data.ALL.star)
boot.median.AML[i] <- median(data.AML.star)
}
quantile(boot.mean.ALL,c(0.025,0.975))
quantile(boot.mean.AML,c(0.025,0.975))
quantile(boot.var.ALL, c(0.025,0.975))
quantile(boot.var.AML, c(0.025,0.975))
quantile(boot.median.ALL,c(0.025,0.975))
quantile(boot.median.AML,c(0.025,0.975))
# (b)
# mean
ci.mean.ALL <- mean(Zyxin.ALL) + qt(c(0.025,0.975), df=length.ALL-1)*sd(Zyxin.ALL)/sqrt(length.ALL)
ci.mean.AML <- mean(Zyxin.AML) + qt(c(0.025,0.975), df=length.AML-1)*sd(Zyxin.AML)/sqrt(length.AML)
ci.mean.ALL
ci.mean.AML
# variance
ci.variance.ALL <- c (var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.975, df=length.ALL-1),
var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.025, df=length.ALL-1))
ci.variance.AML <- c (var(Zyxin.AML)*(length.AML-1)/ qchisq(0.975, df=length.AML-1),
var(Zyxin.AML)*(length.AML-1)/ qchisq(0.025, df=length.AML-1))
ci.variance.ALL
ci.variance.AML
# lecture page 39
# problem 4
MCsim <- function(nsim, lamb){
dataset <- matrix(rpois(nsim*50, lambda = lamb), nrow = nsim)
t0.05 <- qt(0.05, 49)
t0.95 <- -t0.05
kai0.95 <- qchisq(0.95, 49)
kai0.05 <- qchisq(0.05, 49)
means <- apply(dataset, 1, mean)
vars <- apply(dataset, 1, var)
interval.method1 <- matrix(data=NA, nrow=nsim, ncol=2)
interval.method2 <- matrix(data=NA, nrow=nsim, ncol=2)
counter1 <- 0
counter2 <- 0
for (i in 1:nsim) {
interval.method1[i,] <- means[i] + c( t0.05*sqrt(means[i]/50) , t0.95*sqrt(means[i]/50))
interval.method2[i,] <- 49*vars[i]/ ( c( kai0.95, kai0.05) )
if (interval.method1[i,1] <= lamb && interval.method1[i,2] >= lamb) {
counter1 <- counter1 + 1
}
if (interval.method2[i,1] <= lamb && interval.method2[i,2] >= lamb) {
counter2 <- counter2 + 1
}
}
print(counter1/nsim)
print(counter2/nsim)
}
# problem 1
# ln(L) = nln(theta) - n(theta)*(mean of X)
# qiu dao shu
# 1/theta - mean of X = 0
# theta = 1/ mean of X
obs <- c(1.636, 0.374, 0.534, 3.015, 0.932, 0.179)
nloglik <- function(theta) {
-sum(log(dexp(obs, rate = theta)))
}
optim(par = 1, nloglik)
1/mean(obs)
# problem 2
# (a) m = sample mean = 100.8
100.8+qt(0.1, df=52)*(12.4/sqrt(53))
# 98.5891 ~ Inf
# problem 3
data(golub, package = 'multtest')
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
Zyxin.ALL <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "ALL"]
Zyxin.AML <- golub[ grep("Zyxin", golub.gnames[,2]), gol.fac == "AML"]
length.ALL <- length(Zyxin.ALL)
length.AML <- length(Zyxin.AML)
# (a) and (c)
nboot <- 1000
boot.mean.ALL <- rep(NA, nboot)
boot.mean.AML <- rep(NA, nboot)
boot.var.ALL <- rep(NA, nboot)
boot.var.AML <- rep(NA, nboot)
boot.median.ALL <- rep(NA, nboot)
boot.median.AML <- rep(NA, nboot)
for (i in 1:nboot) {
data.ALL.star <- Zyxin.ALL[sample(1:length.ALL, replace=TRUE)]
data.AML.star <- Zyxin.AML[sample(1:length.AML, replace=TRUE)]
boot.mean.ALL[i] <- mean(data.ALL.star)
boot.mean.AML[i] <- mean(data.AML.star)
boot.var.ALL[i] <- var(data.ALL.star)
boot.var.AML[i] <- var(data.AML.star)
boot.median.ALL[i] <- median(data.ALL.star)
boot.median.AML[i] <- median(data.AML.star)
}
quantile(boot.mean.ALL,c(0.025,0.975))
quantile(boot.mean.AML,c(0.025,0.975))
quantile(boot.var.ALL, c(0.025,0.975))
quantile(boot.var.AML, c(0.025,0.975))
quantile(boot.median.ALL,c(0.025,0.975))
quantile(boot.median.AML,c(0.025,0.975))
# (b)
# mean
ci.mean.ALL <- mean(Zyxin.ALL) + qt(c(0.025,0.975), df=length.ALL-1)*sd(Zyxin.ALL)/sqrt(length.ALL)
ci.mean.AML <- mean(Zyxin.AML) + qt(c(0.025,0.975), df=length.AML-1)*sd(Zyxin.AML)/sqrt(length.AML)
ci.mean.ALL
ci.mean.AML
# variance
ci.variance.ALL <- c (var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.975, df=length.ALL-1),
var(Zyxin.ALL)*(length.ALL-1)/ qchisq(0.025, df=length.ALL-1))
ci.variance.AML <- c (var(Zyxin.AML)*(length.AML-1)/ qchisq(0.975, df=length.AML-1),
var(Zyxin.AML)*(length.AML-1)/ qchisq(0.025, df=length.AML-1))
ci.variance.ALL
ci.variance.AML
# lecture page 39
# problem 4
MCsim <- function(nsim, lamb){
dataset <- matrix(rpois(nsim*50, lambda = lamb), nrow = nsim)
t0.05 <- qt(0.05, 49)
t0.95 <- -t0.05
kai0.95 <- qchisq(0.95, 49)
kai0.05 <- qchisq(0.05, 49)
means <- apply(dataset, 1, mean)
vars <- apply(dataset, 1, var)
interval.method1 <- matrix(data=NA, nrow=nsim, ncol=2)
interval.method2 <- matrix(data=NA, nrow=nsim, ncol=2)
counter1 <- 0
counter2 <- 0
for (i in 1:nsim) {
interval.method1[i,] <- means[i] + c( t0.05*sqrt(means[i]/50) , t0.95*sqrt(means[i]/50))
interval.method2[i,] <- 49*vars[i]/ ( c( kai0.95, kai0.05) )
if (interval.method1[i,1] <= lamb && interval.method1[i,2] >= lamb) {
counter1 <- counter1 + 1
}
if (interval.method2[i,1] <= lamb && interval.method2[i,2] >= lamb) {
counter2 <- counter2 + 1
}
}
print(counter1/nsim)
print(counter2/nsim)
}
MCsim(1000, 0.1)
MCsim(1000, 1)
MCsim(1000, 10)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata[,6][Mydata[,6]=="Present"] <- 1
Mydata[,6][Mydata[,6]=="Absent"] <- 0
Mydata <- Mydata[,-1]
Mydata[,6] <- as.integer(Mydata[,6])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata[,6][Mydata[,6]=="Present"] <- 1
Mydata[,6][Mydata[,6]=="Absent"] <- 0
Mydata <- Mydata[,-1]
Mydata[,6] <- as.integer(Mydata[,6])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata[,6][Mydata[,6]=="Present"] <- 1
Mydata[,6][Mydata[,6]=="Absent"] <- 0
Mydata <- Mydata[,-1]
Mydata[,6] <- as.integer(Mydata[,6])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
round(cor(trainSet), digits=2)
pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 3,         # Keep 3 best models
#          plotty = F, report = F,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
#glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
round(cor(trainSet), digits=2)
pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 3,         # Keep 3 best models
#          plotty = F, report = F,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
#glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
fit.glmpath
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
# find parameter value that minimizes cross-validated error
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
cv.s
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
View(pred.glmpath.train)
response[train]
true=response[train]
pred.glmpath.train > 0.5
sum(response[train])
