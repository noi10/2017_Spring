Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- factor(Mydata[,5])
#Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
#format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
#format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
#round(cor(trainSet), digits=2)
#pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
print(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
#glmulti.summary <- summary(glmulti.logistic.out)
#glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
#Mydata[,5] <- factor(Mydata[,5])
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
#format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
#format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
#round(cor(trainSet), digits=2)
#pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
print(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
#glmulti.summary <- summary(glmulti.logistic.out)
#glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
#Mydata[,5] <- factor(Mydata[,5])
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
#format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
#format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
#round(cor(trainSet), digits=2)
#pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
print(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- factor(Mydata[,5])
#Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
#format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
#format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
#round(cor(trainSet), digits=2)
#pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
print(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 2
Mydata[,5][Mydata[,5]=="Absent"] <- 1
Mydata[,5] <- factor(Mydata[,5])
#Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
#format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
#format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
#round(cor(trainSet), digits=2)
#pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
print(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
#######################################################################
#                   Example of variable selection
#######################################################################
library(faraway)
data(pima)
?pima
head(pima)
# ---------------------Fit the full model-----------------------------
fit <- glm(test ~., family=binomial, data=pima)
summary(fit)
# -------------------------Compare nested models-------------------------
summary(fit)
# stepwise variable selection based on AIC
# ('k' distinguishes AIC and BIC)
step.aic <- step(fit, k=2, trace=F)
step.aic$anova
# stepwise variable selection based on BIC
step.bic <- step(fit, k=log(nrow(pima)), trace=F)
step.bic$anova
#######################################################################
#                  ROC curves
#######################################################################
# --------------Predictive ability on the training set-------------------------
library(ROCR)
# select training set (as example, here only use 1/4 of the data to build the model)
train <- sample(x=1:nrow(pima), size=nrow(pima)/4)
# fit the full model on the training dataset
fit.train <- glm(test ~., family=binomial, data=pima[train,])
summary(fit.train)
summary(fit)
# calculate predicted probabilities on the same training set
scores <- predict(fit.train, newdata=pima[train,], type="response")
# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=pima[train,]$test )
perf <- performance(pred, "tpr", "fpr")
# plot the ROC curve
plot(perf, colorize=F, main="In-sample ROC curve")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# --------------Evalate the predictive ability on the validation set------------
# make prediction on the validation dataset
scores <- predict(fit.train, newdata=pima[-train,], type="response")
pred <- prediction( scores, labels=pima[-train,]$test )
perf <- performance(pred, "tpr", "fpr")
# overlay the line for the ROC curve
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
View(pima)
#######################################################################
#######################################################################
#                 L O G I S T I C   R E G R E S S I O N
#######################################################################
#######################################################################
#######################################################################
#                       Example of Bernouilli distribution
#                         (data recordered per subect)
#######################################################################
#setwd("/Users/ovitek/Dropbox/Olga/Teaching/CS6140/Spring17/LectureNotes/4_logisticRegression")
X <- read.table("smokingAndObesity.txt", sep=" ", as.is=TRUE, header=TRUE)
X <- X[order(X$age),]
# factor for 'smoking status'
X$smokeF <- factor(X$smoke)
head(X)
# create a proper binary response for 'overweight'
table(X$over_wt)
X$over_wtF <- factor(abs(X$over_wt - 2), levels=c(0,1))
table(X$over_wtF)
head(X)
# smoking as predictor
fit<- glm(over_wtF ~ smokeF, family=binomial, data=X)
summary(fit)
coef(fit)
# add age
fit<- glm(over_wtF ~ age + smokeF + age*smokeF, family=binomial, data=X)
summary(fit)
knitr::opts_chunk$set(echo = TRUE)
################################################################
# Variable selection with glmpath
################################################################
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
#######################################################################
#######################################################################
#                   L O G I S T I C   R E G R E S S I O N
#                               GLMpath
#######################################################################
#######################################################################
library(faraway)
data(pima)
head(pima)
set.seed(123)
#----------------Separate predictors from the response------------------
predictors <- pima[,1:8]
response <- pima[,9]
#---Partition the dataset into the training and the validation dataset---
train <- sample(x=1:nrow(pima), size=nrow(pima)/4)
################################################################
# Stepwise variable selection
################################################################
# ---------------------Fit the full model-----------------------------
fit.train <- glm(test ~., family=binomial, data=pima[train,])
summary(fit.train)
exp(coef(fit.train))
predict(fit.train, type="response")
residuals(fit.train, type="deviance")
# --------------------Stepwise variable selection---------------------
# stepwise variable selection based on AIC
step.train <- step(fit.train, k=2, trace=F)
step.train$anova
step.fit.train <- glm(test ~ pregnant+glucose+insulin+bmi, family=binomial, data=pima[train,])
summary(step.fit.train)
################################################################
# Variable selection with glmpath
################################################################
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
# find parameter value that minimizes cross-validated error
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
response[train]
################################################################
# Variable selection with glmpath
################################################################
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
# find parameter value that minimizes cross-validated error
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
################################################################
# Variable selection with glmpath
################################################################
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
# find parameter value that minimizes cross-validated error
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
################################################################
# Variable selection with glmpath
################################################################
library(glmpath)
# selection of the parameter lambda
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
# find parameter value that minimizes cross-validated error
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
#pred.coef
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
knitr::opts_chunk$set(echo = TRUE)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response")
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response", mode="lambda")
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response", mode="lambda")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, type="response", mode="lambda")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
cv.s
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, mode = "lambda", nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T, mode = "lambda", type="response")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T , type="response")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T , type="response")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T , type="response")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
