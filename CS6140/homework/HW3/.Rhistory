perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
lda.fit <- lda(x=as.matrix(trainSet[,-10]), grouping= trainSet[,10], cv=TRUE)
# prediction on the training set
pred.lda.train <- predict(lda.fit, newx=trainSet[,-10])
table(true= response[train], predicted=pred.lda.train$class)
# prediction on the validation set
pred.lda.test <- predict(lda.fit, newx=testSet[,-10])
table(true= testSet[,10], predicted=pred.lda.test$class)
predict.lda <- predict(lda.fit, newx=testSet[,-10])
table(true= response[-train], predicted=predict.lda$class)
# ROC on the training set
scores <- predict(lda.fit, newdata= trainSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="LDA")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
scores <- predict(lda.fit, newdata= testSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= testSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
lda.fit <- lda(x=as.matrix(trainSet[,-10]), grouping= trainSet[,10], cv=TRUE)
# prediction on the training set
pred.lda.train <- predict(lda.fit, newx=trainSet[,-10])
table(true= response[train], predicted=pred.lda.train$class)
# prediction on the validation set
pred.lda.test <- predict(lda.fit, newx=testSet[,-10])
table(true= testSet[,10], predicted=pred.lda.test$class)
# ROC on the training set
scores <- predict(lda.fit, newdata= trainSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="LDA")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
scores <- predict(lda.fit, newdata= testSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= testSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
glmulti.logistic.out <-
glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic",
confsetsize=512, plotty = F, report = F, family = binomial, method = "h")
plot(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
print(glmulti.logistic.out)
#allsubset.fit <- glm(chd ~ 1 + tobacco + ldl + famhist + typea + age, family=binomial, data=trainSet)
#summary(allsubset.fit)
allsubset.fit <- glm(chd ~ 1 + tobacco + ldl + famhist + typea + age, family=binomial, data=trainSet)
summary(allsubset.fit)
# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=trainSet, type="response")
# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)
# plot the ROC curve
plot(perf, colorize=F, main="All subset")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=testSet, type="response")
# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=testSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)
# plot the ROC curve
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T , type="response")
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)
# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s,
mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)
#### ROC
# ROC on the training set
pred <- prediction( predictions=pred.glmpath.train, labels=response[train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="glmpath")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred <- prediction( predictions=pred.glmpath.valid, labels=response[-train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
knitr::opts_chunk$set(echo = TRUE)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
library(ROCR)
library(MASS)
library(pamr)
setwd("C:/Users/lenovo/Desktop/2017_Spring/CS6140/homework/HW3")
#setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.197)
pamr.confusion(fit.cv.pamr, threshold=4.928)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr, xlim = c(1,10))
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
#pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
knitr::opts_chunk$set(echo = TRUE)
#######################################################################
# Nearest shrunken centroids
#######################################################################
#Prediction Analysis of Microarrays for R
library(pamr)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(pimaTrain[,-9])), y=pimaTrain[,9])
knitr::opts_chunk$set(echo = TRUE)
#setwd("/Users/ovitek/Dropbox/Olga/Teaching/CS6140/Spring17/LectureNotes/5_generative")
library(Biobase)
########################################################################
#                   Example of generative classifiers
#######################################################################
library(faraway)
data(pima)
?pima
head(pima)
################################################################
# Separation of the training and the validation datasets
################################################################
# select training set (as example, here only use 1/4 of the data to build the model)
# set seed for reproducible result of random sampling
set.seed(123)
train <- sample(x=1:nrow(pima), size=nrow(pima)/4)
pimaTrain <- pima[train,]
pimaValid <- pima[-train,]
################################################################
#                 				LDA
################################################################
# LDA
library(MASS)
fit.lda <- lda(x=as.matrix(pimaTrain[,-9]), grouping= pimaTrain[,9], cv=TRUE)
# prediction on the training set
predict.lda <- predict(fit.lda, pimaTrain[,-9])
table(true= pimaTrain[,9], predict=predict.lda$class)
# alternative visualization
library(gmodels)
CrossTable(predict.lda$class,pimaTrain[,9])
# ROC on the training set
library(ROCR)
scores <- predict(fit.lda, newdata= pimaTrain[,-9])$posterior[,2]
pred <- prediction( scores, labels= pimaTrain$test )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, main="LDA")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
# prediction on the validation set
predict.lda <- predict(fit.lda, pimaValid[,-9])
table(true= pimaValid[,9], predict=predict.lda$class)
scores <- predict(fit.lda, newdata= pimaValid[,-9])$posterior[,2]
pred <- prediction( scores, labels= pimaValid$test )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, main="LDA", add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# QDA training
fit.qda <- qda(x=as.matrix(pimaTrain[,-9]), grouping= pimaTrain[,9], cv=TRUE)
predict.qda <- predict(fit.qda, pimaTrain[,-9])
table(true= pimaTrain[,9], predict=predict.qda$class)
# QDA validation
predict.qda <- predict(fit.qda, pimaValid[,-9])
table(true= pimaValid[,9], predict=predict.qda$class)
# *** In the followin examples, I only use a subset of evaluations for brievity
# *** Please use them fully for homeworks/projects
################################################################
#                 			Naive Bayes
################################################################
library(e1071)
fit.nb <- naiveBayes(pimaTrain[,-9], as.factor(pimaTrain[,9]))
# Naive Bayes training
predict.nb <- predict(fit.nb, pimaTrain[,-9], type="class")
table(true= pimaTrain[,9], predict=predict.nb)
#######################################################################
# Nearest shrunken centroids
#######################################################################
#Prediction Analysis of Microarrays for R
library(pamr)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(pimaTrain[,-9])), y=pimaTrain[,9])
pamrValid <- list(x=t(as.matrix(pimaValid[,-9])), y=pimaValid[,9])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.197)
pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
#######################################################################
# Nearest shrunken centroids
#######################################################################
#Prediction Analysis of Microarrays for R
library(pamr)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(pimaTrain[,-9])), y=pimaTrain[,9])
pamrValid <- list(x=t(as.matrix(pimaValid[,-9])), y=pimaValid[,9])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
summary(fit.cv.pamr)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.197)
pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
#######################################################################
# Nearest shrunken centroids
#######################################################################
#Prediction Analysis of Microarrays for R
library(pamr)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(pimaTrain[,-9])), y=pimaTrain[,9])
pamrValid <- list(x=t(as.matrix(pimaValid[,-9])), y=pimaValid[,9])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
summary(fit.cv.pamr)
print(fit.cv.pamr)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.197)
pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
print(pamr.plotcv)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
View(trainSet)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
fit.pamr
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
#pamr.confusion(fit.cv.pamr, threshold=0.197)
#pamr.confusion(fit.cv.pamr, threshold=4.928)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
fit.pamr
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.128)
pamr.confusion(fit.cv.pamr, threshold=0.640)
# Refit the classifier on the full dataset, but using the threshold
#fit.pamr <- pamr.train(pamrTrain, threshold=0.197)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= pimaTrain$chd)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=1.380, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=1.380, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])
# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
fit.pamr
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
# Manually select the threshold depending on the plots and on the confusion matrix
pamr.plotcv(fit.cv.pamr)
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.128)
pamr.confusion(fit.cv.pamr, threshold=0.640)
# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.640)
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set
pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
unlist(attributes(performance(pred, "auc"))$y.values)
