---
title: "Gu_HW3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
library(ROCR)
library(MASS)
library(pamr)
#setwd("C:/Users/lenovo/Desktop/2017_Spring/CS6140/homework/HW3")
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
```
## 1 Data preparation and exploration
### (a) Select the training set
I downloaded the data from the website and read it using read.table().
Then I used sample function to partition the dataset into a training set and a validatoin set of equal size.
Notice that there is a categorical feature famhist, I set "Present" to be 1 and "Absent" to be 0 accordingly.
```{r}
set.seed(123)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
testSet <- Mydata[-train,]
```

### (b) Data exploration
```{r}
options(width=100)
# missing values
sum(is.na(Mydata))

# outliers
boxplot(trainSet, las = 2)
```

As we can see above, there is no missing value in our dataset.
Besides, the boxplot indicates that there seems to be some outliers according to sbp and alchohol. However, different from the rest doesn't necessarily mean wrong as we discussed in class. Since data is scarce, we decided not to remove these "outliers". After this, we can apply one-variable and two-variable summary.


#### one variable summary

simple one variable summary
```{r}
# One variable summary
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
```


#### two variable summary
```{r}
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)

# Two variable summary
round(cor(trainSet), digits=2)
pairs(trainSet, col=c("green", "blue"))
```

famhist is the only categorical predictor in our dataset which has the value of either 0 or 1.
adiposity vs obesity, adiposity vs age are the paris which are highly correlated predictors. We will see how our models select variables later.

## 2 logistic regression 
#### All subset selection
```{r}
glmulti.logistic.out <- 
  glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", 
          confsetsize=512, plotty = F, report = F, family = binomial, method = "h")
plot(glmulti.logistic.out)
print(glmulti.logistic.out)


tmp <- weightable(glmulti.logistic.out)
tmp <- tmp[tmp$aic <= min(tmp$aic) + 2, ]
tmp
```

Here we used glmulti package which is similar as bestglm to do variable selection. With level = 1, we say that we didn't consider statistical interactions first. Thus, there are $2^9=512$ possible models to consider here. With crit="aic", we selected the information criterion to be Akaike Information Criterion.

The plot above show the AIC values of all 512 models. The horizaontal read line differentiates between models in which AIC is less versus more than 2 units away from that of the best model with loweast AIC. 

The output above shows that there are 7 models whose AIC is less than 2 units away from that of the best model.
And we stored them in tmp and listed them out.


```{r}
summary(glmulti.logistic.out@objects[[1]])

allsubset.fit <- glm(chd ~ 1 + tobacco + ldl + famhist + typea + age, family=binomial, data=trainSet)
summary(allsubset.fit)
```

We see that the best model includes tobacco, ldl, famhist, typea and age as predictors. Highly correlated pairs adiposity vs obesity and adiposity vs age are not involved. The coeffients are reported above.


## 3 LDA 

```{r}
lda.fit <- lda(x=as.matrix(trainSet[,-10]), grouping= trainSet[,10], cv=TRUE)

lda.fit

plot(lda.fit)
```
The LDA output indicates that estimated $\pi_{1}=0.6450$ and $\pi_{2}=0.3550$.
Also, it provides group means which were used by LDA as estimates of $\mu_{k}$.

The coefficients of linear discriminants output provides the linear combination of all the 9 predictors that are used to form LDA decision rule. If $\beta^TX$ is large, then the LDA classifier will predict chd = 1, and if it is small, then the LDA classifier will predict chd = 0.

The plot() function produces plots of the linear discriminants which are approximately bell-shaped, obtained by computing $\beta^TX$ for each of the training observations. 





## logistic regression with Lasso regularization
In this section, we used glmpath package to implement lasso regularization.

#### Coefficients path
```{r}
# fit the model with glmpath to plot the path
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
                       y=response[train], family=binomial)

# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")

```
From the coefficients path, we concluded that the rank of these 9 predictors regarding "importance" is age, famhist, typea, ldl, tobacco, adiposity, alcohol, sbp and obesity (from high to low).

#### Cross-validated predicted error plot
```{r results="hide"}
# cross-validated prediction on the training set using prediction error
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
                  y=response[train], 
                  family=binomial, nfold=10, plot.it=T , type="response")
cv.glmpath
cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]
abline(v=cv.s, lty = 2, col = "red")

```
We used a dash red line to mark the "norm fraction of lambda" that holds minimum cross-validation errors. Note that the curve of minus log-likelihood vs norm fraction is smooth while the one of cross-validation errors vs norm fraction is rugged. That's because there is probability that the errors remain the same even if the model changes slightly due to lambda.


#### regularization parameter
```{r}
# get the cv.s with lowest cross-validated predicted error
cv.s
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")

```

For this Lasso model, we got the norm fraction of lambda `r cv.s`.
And the coefficients of predictors are showed. We find that adiposity and obesity are dropped because they are less "important".

#### Fit without lambda
```{r}
```

```{r logistic prediction}
# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=trainSet, type="response")

# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)

# plot the ROC curve
plot(perf, colorize=F, main="All subset")
# print out the area under the curve
allsubset.train <- unlist(attributes(performance(pred, "auc"))$y.values)

# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=testSet, type="response")

# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=testSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)
# plot the ROC curve
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
allsubset.test <- unlist(attributes(performance(pred, "auc"))$y.values)
```
```{r lda prediction}
# prediction on the training set
pred.lda.train <- predict(lda.fit, newx=trainSet[,-10])
table(true= response[train], predicted=pred.lda.train$class)

# prediction on the validation set
pred.lda.test <- predict(lda.fit, newx=testSet[,-10])
table(true= testSet[,10], predicted=pred.lda.test$class)


# ROC on the training set
scores <- predict(lda.fit, newdata= trainSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="LDA")
# print out the area under the curve
lda.train <- unlist(attributes(performance(pred, "auc"))$y.values)


# ROC on the validation set
scores <- predict(lda.fit, newdata= testSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= testSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
lda.test <- unlist(attributes(performance(pred, "auc"))$y.values)

```
```{r Lasso prediction}
# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s, 
			mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)

# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s, 
			mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)

#### ROC
# ROC on the training set
pred <- prediction( predictions=pred.glmpath.train, labels=response[train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="glmpath")
# print out the area under the curve
lasso.train <- unlist(attributes(performance(pred, "auc"))$y.values)


# ROC on the validation set
pred <- prediction( predictions=pred.glmpath.valid, labels=response[-train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
lasso.test <- unlist(attributes(performance(pred, "auc"))$y.values)
```



#### nearest shrunken centroids
```{r results="hide" }
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])

# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
fit.pamr
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
```
```{r fig.width=8.5}

error.bars <- function(x, lower, upper, length=0.1,...){
   arrows(x,lower, x, upper, angle=90, code=3, length=length, ...)
}
fit <- fit.cv.pamr
par(mar = c(5, 4.5, 5, 0.5))
par(mfrow = c(1, 2))
n <- nrow(fit$yhat)
y <- fit$y
if (!is.null(fit$newy)) {
    y <- fit$newy[fit$sample.subset]
}
nc <- length(table(y))
nfolds <- length(fit$folds)
err <- matrix(NA, ncol = ncol(fit$yhat), nrow = nfolds)
temp <- matrix(y, ncol = ncol(fit$yhat), nrow = n)
ni <- rep(NA, nfolds)
for (i in 1:nfolds) {
    ii <- fit$folds[[i]]
    ni[i] <- length(fit$folds[[i]])
    err[i, ] <- apply(temp[ii, ] != fit$yhat[ii, ], 2, sum)/ni[i]
}
se <- sqrt(apply(err, 2, var)/nfolds)
plot(fit$threshold, fit$error, ylim = c(-0.1, 1), xlab = "Value of threshold",ylab = "Misclassification Error", type = "n", yaxt = "n", cex=0.6)

axis(3, at = fit$threshold, labels = paste(fit$size), srt = 90, adj = 0)
mtext("Number of variables", 3, 2, cex = 1)
axis(2, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0))
lines(fit$threshold, fit$error, col = 2)
o <- fit$err == min(fit$err)
points(fit$threshold[o], fit$error[o], pch = "x")
error.bars(fit$threshold, fit$err - se, fit$err + se)
err2 <- matrix(NA, nrow = length(unique(y)), ncol = length(fit$threshold))
for (i in 1:(length(fit$threshold) - 1)) {
    s <- pamr.confusion(fit, fit$threshold[i], extra = FALSE)
    diag(s) <- 0
    err2[, i] <- apply(s, 1, sum)/table(y)
}
plot(fit$threshold, err2[1, ], ylim = c(-0.1, 1.1), xlab = "Value of threshold ", 
    ylab = "Misclassification Error", type = "n", yaxt = "n")
mtext("Number of variables", 3, 2, cex = 1)
axis(3, at = fit$threshold, labels = paste(fit$size), srt = 90, adj = 0)
axis(2, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0))
for (i in 1:nrow(err2)) {
    lines(fit$threshold, err2[i, ], col = i + 1)
}
legend(2.5, 0.4, dimnames(table(y))[[1]], col = (2:(nc + 1)), lty = 1)
title("Nearest Shrunken Centroids", line = -1, outer=TRUE, adj=0.6)
```


```{r}
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.128)
pamr.confusion(fit.cv.pamr, threshold=0.640)

# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.640)

```


```{r}
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set

pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
nearest.train <- unlist(attributes(performance(pred, "auc"))$y.values)

# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
nearest.test <- unlist(attributes(performance(pred, "auc"))$y.values)


```

```{r}
t(matrix(c(allsubset.train, allsubset.test, lasso.train, lasso.test, lda.train, lda.test, nearest.train, nearest.test), nrow = 2, dimnames = list(c("trainSet","testSet"),c("allsubset", "lasso", "lda", "nearest"))))
```
