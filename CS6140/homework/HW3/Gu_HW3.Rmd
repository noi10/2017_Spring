---
title: "Gu_HW3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
library(ROCR)
library(MASS)
library(pamr)
setwd("C:/Users/lenovo/Desktop/2017_Spring/CS6140/homework/HW3")
#setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/homework/HW3")
```

```{r}
set.seed(123)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
testSet <- Mydata[-train,]
```

```{r}
options(width=100)
sum(is.na(Mydata))

one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary

format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)

round(cor(trainSet), digits=2)
pairs(trainSet, col=c("green", "blue"))
```

#### All subset selection
```{r}
glmulti.logistic.out <- 
  glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", 
          confsetsize=512, plotty = F, report = F, family = binomial, method = "h")
plot(glmulti.logistic.out)
summary(glmulti.logistic.out@objects[[1]])
print(glmulti.logistic.out)

#allsubset.fit <- glm(chd ~ 1 + tobacco + ldl + famhist + typea + age, family=binomial, data=trainSet)
#summary(allsubset.fit)

allsubset.fit <- glm(chd ~ 1 + tobacco + ldl + famhist + typea + age, family=binomial, data=trainSet)
summary(allsubset.fit)


# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=trainSet, type="response")

# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)

# plot the ROC curve
plot(perf, colorize=F, main="All subset")
# print out the area under the curve
allsubset.train <- unlist(attributes(performance(pred, "auc"))$y.values)

# calculate predicted probabilities on the same training set
scores <- predict(allsubset.fit, newdata=testSet, type="response")

# compare predicted probabilities to labels, for varying probability cutoffs
pred <- prediction(scores, labels=testSet$chd )
perf <- performance(pred, "tpr", "fpr")
table(true=response[train], predicted = scores > 0.5)
# plot the ROC curve
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
allsubset.test <- unlist(attributes(performance(pred, "auc"))$y.values)
```



With level = 1, we stick to models with main effects only. This implies that there are 
2^7=128 possible models in the candidate set to consider. Since I want to keep the results for all these models (the default is to only keep up to 100 model fits), I set confsetsize=128 (or I could have set this to some very large value). With crit="aicc", we select the information criterion (in this example: the AICc or corrected AIC) that we would like to compute for each model and that should be used for model selection and multimodel inference. For more information about the AIC (and AICc), see, for example, the entry for the Akaike Information Criterion on Wikipedia. As the function runs, you should receive information about the progress of the model fitting. Fitting the 128 models should only take a few seconds.





#### LDA
```{r}
lda.fit <- lda(x=as.matrix(trainSet[,-10]), grouping= trainSet[,10], cv=TRUE)

plot(lda.fit)
# prediction on the training set
pred.lda.train <- predict(lda.fit, newx=trainSet[,-10])
table(true= response[train], predicted=pred.lda.train$class)

# prediction on the validation set
pred.lda.test <- predict(lda.fit, newx=testSet[,-10])
table(true= testSet[,10], predicted=pred.lda.test$class)


# ROC on the training set
scores <- predict(lda.fit, newdata= trainSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= trainSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="LDA")
# print out the area under the curve
lda.train <- unlist(attributes(performance(pred, "auc"))$y.values)


# ROC on the validation set
scores <- predict(lda.fit, newdata= testSet[,-10])$posterior[,2]
pred <- prediction( scores, labels= testSet$chd )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
lda.test <- unlist(attributes(performance(pred, "auc"))$y.values)

```


#### regularization
```{r results="hide"}
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
                             y=response[train], 
                             family=binomial, nfold=10, plot.it=T)

# cross-validated prediction on the training set
fit.cv.glmpath1 <- cv.glmpath(x=as.matrix(predictors[train,]),
                  y=response[train], 
                  family=binomial, nfold=10, plot.it=T , type="response")

cv.s <- fit.cv.glmpath$fraction[which.min(fit.cv.glmpath$cv.error)]

# refit the model without cross-validation
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
                       y=response[train], family=binomial)
predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
```

```{r}
# plot the path
par(mfrow=c(1,1), mar=c(4,4,4,8))
plot(fit.glmpath, xvar="lambda")

# in-sample predictive accuracy
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.train <- predict(fit.glmpath, newx=as.matrix(predictors[train,]), s=cv.s, 
			mode="norm.fraction", type="response")
table(true=response[train], predicted=pred.glmpath.train > 0.5)

# predictive accuracy on a validation set
pred.coef <- predict(fit.glmpath, s=cv.s, mode="norm.fraction", type="coefficients")
pred.coef
pred.glmpath.valid <- predict(fit.glmpath, newx=as.matrix(predictors[-train,]), s=cv.s, 
			mode="norm.fraction", type="response")
table(true=response[-train], predicted=pred.glmpath.valid > 0.5)

#### ROC
# ROC on the training set
pred <- prediction( predictions=pred.glmpath.train, labels=response[train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="glmpath")
# print out the area under the curve
lasso.train <- unlist(attributes(performance(pred, "auc"))$y.values)


# ROC on the validation set
pred <- prediction( predictions=pred.glmpath.valid, labels=response[-train] )
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
lasso.test <- unlist(attributes(performance(pred, "auc"))$y.values)
```



#### nearest shrunken centroids
```{r results="hide" }
# Reformat the dataset for parm
pamrTrain <- list(x=t(as.matrix(trainSet[,-10])), y=trainSet[,10])
pamrValid <- list(x=t(as.matrix(testSet[,-10])), y=testSet[,10])

# Fit the classifier on the entire training set
fit.pamr <- pamr.train(pamrTrain)
fit.pamr
# Use cross-validation to select the best regularization parameter
fit.cv.pamr <- pamr.cv(fit.pamr, pamrTrain)
```
```{r fig.width=8.5}

error.bars <- function(x, lower, upper, length=0.1,...){
   arrows(x,lower, x, upper, angle=90, code=3, length=length, ...)
}
fit <- fit.cv.pamr
par(mar = c(5, 4.5, 5, 0.5))
par(mfrow = c(1, 2))
n <- nrow(fit$yhat)
y <- fit$y
if (!is.null(fit$newy)) {
    y <- fit$newy[fit$sample.subset]
}
nc <- length(table(y))
nfolds <- length(fit$folds)
err <- matrix(NA, ncol = ncol(fit$yhat), nrow = nfolds)
temp <- matrix(y, ncol = ncol(fit$yhat), nrow = n)
ni <- rep(NA, nfolds)
for (i in 1:nfolds) {
    ii <- fit$folds[[i]]
    ni[i] <- length(fit$folds[[i]])
    err[i, ] <- apply(temp[ii, ] != fit$yhat[ii, ], 2, sum)/ni[i]
}
se <- sqrt(apply(err, 2, var)/nfolds)
plot(fit$threshold, fit$error, ylim = c(-0.1, 1), xlab = "Value of threshold",ylab = "Misclassification Error", type = "n", yaxt = "n", cex=0.6)

axis(3, at = fit$threshold, labels = paste(fit$size), srt = 90, adj = 0)
mtext("Number of variables", 3, 2, cex = 1)
axis(2, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0))
lines(fit$threshold, fit$error, col = 2)
o <- fit$err == min(fit$err)
points(fit$threshold[o], fit$error[o], pch = "x")
error.bars(fit$threshold, fit$err - se, fit$err + se)
err2 <- matrix(NA, nrow = length(unique(y)), ncol = length(fit$threshold))
for (i in 1:(length(fit$threshold) - 1)) {
    s <- pamr.confusion(fit, fit$threshold[i], extra = FALSE)
    diag(s) <- 0
    err2[, i] <- apply(s, 1, sum)/table(y)
}
plot(fit$threshold, err2[1, ], ylim = c(-0.1, 1.1), xlab = "Value of threshold ", 
    ylab = "Misclassification Error", type = "n", yaxt = "n")
mtext("Number of variables", 3, 2, cex = 1)
axis(3, at = fit$threshold, labels = paste(fit$size), srt = 90, adj = 0)
axis(2, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0))
for (i in 1:nrow(err2)) {
    lines(fit$threshold, err2[i, ], col = i + 1)
}
legend(2.5, 0.4, dimnames(table(y))[[1]], col = (2:(nc + 1)), lty = 1)
title("Nearest Shrunken Centroids", line = -1, outer=TRUE, adj=0.6)
```


```{r}
print(fit.cv.pamr)
#Let's compare thresholds=1 and 2 to illustrate the effect of shrinkage
pamr.confusion(fit.cv.pamr, threshold=0.128)
pamr.confusion(fit.cv.pamr, threshold=0.640)

# Refit the classifier on the full dataset, but using the threshold
fit.pamr <- pamr.train(pamrTrain, threshold=0.640)

```


```{r}
# ----------ROC curve using nearest shrunken centroids----------------
# ROC on the training set

pred.pamr.train <- pamr.predict(fit.pamr, newx=pamrTrain$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.train, labels= trainSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=F, main="Nearest shrunken centroids")
# print out the area under the curve
nearest.train <- unlist(attributes(performance(pred, "auc"))$y.values)

# ROC on the validation set
pred.pamr.valid <- pamr.predict(fit.pamr, newx=pamrValid$x, threshold=0.640, type="posterior")[,2]
pred <- prediction(predictions=pred.pamr.valid, labels= testSet$chd)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=T, add=TRUE)
# print out the area under the curve
nearest.test <- unlist(attributes(performance(pred, "auc"))$y.values)


```

```{r}
t(matrix(c(allsubset.train, allsubset.test, lasso.train, lasso.test, lda.train, lda.test, nearest.train, nearest.test), nrow = 2, dimnames = list(c("trainSet","testSet"),c("allsubset", "lasso", "lda", "nearest"))))
```
