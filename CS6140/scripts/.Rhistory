Mydata[,6] <- as.integer(Mydata[,6])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
sum(is.na(Mydata))
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
knitr::opts_chunk$set(echo = TRUE)
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
sum(is.na(Mydata))
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
round(cor(trainSet), digits=2)
pairs(trainSet)
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 3,         # Keep 3 best models
plotty = T, report = T,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
#glmulti.logistic.out@formulas
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 10,         # Keep 10 best models
plotty = T, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
sum(is.na(Mydata))
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
round(cor(trainSet), digits=2)
pairs(trainSet)
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 10,         # Keep 10 best models
plotty = T, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 5,         # Keep 10 best models
plotty = T, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 3,         # Keep 10 best models
plotty = T, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 3,         # Keep 10 best models
plotty = F, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
#glmulti.logistic.out@formulas
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 3,         # Keep 10 best models
plotty = F, report = F,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
#glmulti.logistic.out@formulas
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 3,         # Keep 3 best models
plotty = T, report = T,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <-
glmulti(chd ~ ., data = trainSet,
level = 1,               # Interaction considered
method = "h",            # Exhaustive approach
crit = "aic",            # AIC as criteria
confsetsize = 10,         # Keep 3 best models
plotty = T, report = T,  # No plot or interim reports
fitfunction = "glm",     # glm function
family = binomial)       # binomial family for logistic regression
library(glmulti)
library(glmpath)
library(pastecs)
library(knitr)
Mydata <- read.table("SouthAfricanHeartDisease.txt", sep=",", stringsAsFactors = FALSE, header = TRUE)
Mydata <- Mydata[,-1]
Mydata[,5][Mydata[,5]=="Present"] <- 1
Mydata[,5][Mydata[,5]=="Absent"] <- 0
Mydata[,5] <- as.integer(Mydata[,5])
predictors <- Mydata[,1:9]
response <- Mydata[,10]
sum(is.na(Mydata))
train <- sample(x=1:nrow(Mydata), size=nrow(Mydata)/2)
trainSet <- Mydata[train,]
one.variable.summary <- summary(trainSet, digits=2)
one.variable.summary
format(round(stat.desc(trainSet, basic=F), 2), nsmall = 2)
format(round(stat.desc(trainSet, desc=F), 2), nsmall=2)
round(cor(trainSet), digits=2)
pairs(trainSet)
#glmulti.logistic.out <-
#  glmulti(chd ~ ., data = trainSet,
#          level = 1,               # Interaction considered
#          method = "h",            # Exhaustive approach
#          crit = "aic",            # AIC as criteria
#          confsetsize = 10,         # Keep 3 best models
#          plotty = T, report = T,  # No plot or interim reports
#          fitfunction = "glm",     # glm function
#          family = binomial)       # binomial family for logistic regression
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
#glmulti.summary <- summary(glmulti.logistic.out)
#glmulti.logistic.out@formulas
#summary(glmulti.logistic.out@objects[[1]])
fit.cv.glmpath <- cv.glmpath(x=as.matrix(predictors[train,]),
y=response[train],
family=binomial, nfold=10, plot.it=T)
fit.glmpath <- glmpath(x=as.matrix(predictors[train,]),
y=response[train], family=binomial)
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
glmulti.logistic.out <- glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic", confsetsize=512)
plot(glmulti.logistic.out)
glmulti.logistic.out <-
glmulti(chd~. , data=trainSet, level=1, fitfunction="glm", crit = "aic",
confsetsize=512, plotty = F, report = F)
plot(glmulti.logistic.out)
setwd("C:/Users/Bobo/Desktop/2017_Spring/CS6140/scripts")
X <- read.table("smokingAndObesity.txt", sep=" ", as.is=TRUE, header=TRUE)
X <- X[order(X$age),]
# factor for 'smoking status'
X$smokeF <- factor(X$smoke)
head(X)
table(X$over_wt)
X$over_wtF <- factor(abs(X$over_wt - 2), levels=c(0,1))
table(X$over_wtF)
head(X)
View(X)
fit<- glm(over_wtF ~ smokeF, family=binomial, data=X)
summary(fit)
fit<- glm(over_wtF ~ age + smokeF + age*smokeF, family=binomial, data=X)
summary(fit)
library(faraway)
data(orings)
?orings
orings
library(knitr)
library(multtest)
t.test(golub[2972, gol.fac=="ALL"]-golub[2989, gol.fac=="ALL"], alternative = "less")
library(knitr)
library(multtest)
data(golub)
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
t.test(golub[2972, gol.fac=="ALL"], mu=-0.9, alternative = "greater")
t.test(golub[2972, gol.fac=="ALL"],golub[2972, gol.fac=="AML"])
t.test(golub[2972, gol.fac=="ALL"]-golub[2989, gol.fac=="ALL"], alternative = "less")
t.test(golub[2972, gol.fac=="ALL"], golub[2989, gol.fac=="ALL"]，alternative = "less" )
t.test(golub[2972, gol.fac=="ALL"]-golub[2989, gol.fac=="ALL"], alternative = "less")
t.test(golub[2972, gol.fac=="ALL"], golub[2989, gol.fac=="ALL"]，alternative = "less"， paired=T )
res <- golub[2972,] > -0.6
binom.test(sum(res), length(res), p=0.5, alternative = "less")
res <- golub[2972,gol.fac=="ALL"] > -0.6
binom.test(sum(res), length(res), p=0.5, alternative = "less")
# problem 1
library(multtest)
data(golub)
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
# (a)
t.test(golub[2972, gol.fac=="ALL"], mu=-0.9, alternative = "greater")
# (b)
t.test(golub[2972, gol.fac=="ALL"],golub[2972, gol.fac=="AML"])
# (c)
t.test(golub[2972, gol.fac=="ALL"]-golub[2989, gol.fac=="ALL"], alternative = "less")
t.test(golub[2972, gol.fac=="ALL"], golub[2989, gol.fac=="ALL"], alternative = "less", paired=T )
# (d)
res <- golub[2972, gol.fac=="ALL"] < golub[2989, gol.fac=="ALL"]
binom.test(sum(res), length(res), p=1/2, alternative = "greater")
# (e)
res <- golub[2972,gol.fac=="ALL"] > -0.6
binom.test(sum(res), length(res), p=0.5, alternative = "less")
# (f)
resALL <- golub[2972, gol.fac=="ALL"] < -0.6
obsALL <- sum(resALL); nALL <- length(resALL)
resAML <- golub[2972, gol.fac=="AML"] < -0.6
obsAML <- sum(resAML); nAML <- length(resAML)
prop.test( x=c(obsALL, obsAML), n=c( nALL, nAML ), alternative="two.sided")
# problem 2
rm(list=ls())
# (a)
2000*0.05
# (b)
pbinom(89, size=2000, prob=0.05)
# problem 3
rm(list=ls())
#(a)
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.3, df=19) & tstat.sim < qt(0.4, df=19))
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.9, df=19) )
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
# problem 4
rm(list=ls())
# (a)
data(golub, package = "multtest")
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
p.values <- apply(golub, 1, function(x)
{
xALL = t(as.matrix(x))[,gol.fac=="ALL"]
xAML = t(as.matrix(x))[,gol.fac=="AML"]
t.test(xALL, xAML)$p.value
}
)
p.bon <-p.adjust(p=p.values, method="bonferroni")
p.fdr <-p.adjust(p=p.values, method="fdr")
sum(p.values<0.05)
sum(p.bon<0.05)
sum(p.fdr<0.05)
# (b)
golub.gnames[,2][order(p.values)][1:3]
golub.gnames[,2][order(p.bon)][1:3]
golub.gnames[,2][order(p.fdr)][1:3]
# problem 5
rm(list=ls())
# (a)
# Wald.CI
Wald.CI <- function(x, n, conf.level) {
p <- x/n
z <- qnorm(1-(1-conf.level)/2)
p + c(-1,1)*z*sqrt(p*(1-p)/n)
}
# Wilson.CI
Wilson.CI <- function(x, n, conf.level) {
p <- x/n
z <- qnorm(1-(1-conf.level)/2)
(x+z^2/2)/(n+z^2) + c(-1, 1)*(z*sqrt(n)/(n+z^2))*sqrt(p*(1-p)+z^2/(4*n))
}
# AC.CI
AC.CI <- function(x, n, conf.level) {
z <- qnorm(1-(1-conf.level)/2)
x.s <- x + z^2/2
n.s <- n + z^2
p.s <- x.s/n.s
q.s <- 1-p.s
p.s + c(-1, 1)*z*sqrt(p.s*q.s/n.s)
}
# (b)
n <- 40
p <- 0.2
x.sim <- rbinom(10000, size=n, prob=p)
Wald.sim <- matrix(Wald.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(Wald.sim[1,] < p & Wald.sim[2,] > p)
Wilson.sim <- matrix(Wilson.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(Wilson.sim[1,] < p & Wilson.sim[2,] > p)
AC.sim <- matrix(AC.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(AC.sim[1,] < p & AC.sim[2,] > p)
# problem 1
library(multtest)
data(golub)
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
# (a)
t.test(golub[2972, gol.fac=="ALL"], mu=-0.9, alternative = "greater")
# (b)
t.test(golub[2972, gol.fac=="ALL"],golub[2972, gol.fac=="AML"])
# (c)
t.test(golub[2972, gol.fac=="ALL"]-golub[2989, gol.fac=="ALL"], alternative = "less")
t.test(golub[2972, gol.fac=="ALL"], golub[2989, gol.fac=="ALL"], alternative = "less", paired=T )
# (d)
res <- golub[2972, gol.fac=="ALL"] < golub[2989, gol.fac=="ALL"]
binom.test(sum(res), length(res), p=1/2, alternative = "greater")
# (e)
res <- golub[2972,gol.fac=="ALL"] > -0.6
binom.test(sum(res), length(res), p=0.5, alternative = "less")
# (f)
resALL <- golub[2972, gol.fac=="ALL"] < -0.6
obsALL <- sum(resALL); nALL <- length(resALL)
resAML <- golub[2972, gol.fac=="AML"] < -0.6
obsAML <- sum(resAML); nAML <- length(resAML)
prop.test( x=c(obsALL, obsAML), n=c( nALL, nAML ), alternative="two.sided")
# problem 2
rm(list=ls())
# (a)
2000*0.05
# (b)
pbinom(89, size=2000, prob=0.05)
# problem 3
rm(list=ls())
#(a)
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.3, df=19) & tstat.sim < qt(0.4, df=19))
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.9, df=19) )
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
# problem 4
rm(list=ls())
# (a)
data(golub, package = "multtest")
gol.fac <- factor(golub.cl, levels=0:1, labels=c("ALL", "AML"))
p.values <- apply(golub, 1, function(x)
{
xALL = t(as.matrix(x))[,gol.fac=="ALL"]
xAML = t(as.matrix(x))[,gol.fac=="AML"]
t.test(xALL, xAML)$p.value
}
)
p.bon <-p.adjust(p=p.values, method="bonferroni")
p.fdr <-p.adjust(p=p.values, method="fdr")
sum(p.values<0.05)
sum(p.bon<0.05)
sum(p.fdr<0.05)
# (b)
golub.gnames[,2][order(p.values)][1:3]
golub.gnames[,2][order(p.bon)][1:3]
golub.gnames[,2][order(p.fdr)][1:3]
# problem 5
rm(list=ls())
# (a)
# Wald.CI
Wald.CI <- function(x, n, conf.level) {
p <- x/n
z <- qnorm(1-(1-conf.level)/2)
p + c(-1,1)*z*sqrt(p*(1-p)/n)
}
# Wilson.CI
Wilson.CI <- function(x, n, conf.level) {
p <- x/n
z <- qnorm(1-(1-conf.level)/2)
(x+z^2/2)/(n+z^2) + c(-1, 1)*(z*sqrt(n)/(n+z^2))*sqrt(p*(1-p)+z^2/(4*n))
}
# AC.CI
AC.CI <- function(x, n, conf.level) {
z <- qnorm(1-(1-conf.level)/2)
x.s <- x + z^2/2
n.s <- n + z^2
p.s <- x.s/n.s
q.s <- 1-p.s
p.s + c(-1, 1)*z*sqrt(p.s*q.s/n.s)
}
# (b)
n <- 40
p <- 0.2
x.sim <- rbinom(10000, size=n, prob=p)
Wald.sim <- matrix(Wald.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(Wald.sim[1,] < p & Wald.sim[2,] > p)
Wilson.sim <- matrix(Wilson.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(Wilson.sim[1,] < p & Wilson.sim[2,] > p)
AC.sim <- matrix(AC.CI(x.sim, n=n, conf.level=0.95), nrow=2)
mean(AC.sim[1,] < p & AC.sim[2,] > p)
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.3, df=19) & tstat.sim < qt(0.4, df=19))
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
# original one
x.sim <- matrix(rnorm(10000*20, mean=3, sd=4), ncol=20)
tstat <- function(x) (mean(x)-3)/sd(x)*sqrt(length(x))
tstat.sim <- apply(x.sim, 1, tstat)
power.sim <- mean(tstat.sim > qt(0.9, df=19) )
power.sim+c(-1,0,1)*qnorm(0.975)*sqrt(power.sim*(1-power.sim)/10000)
setwd("C:/Users/lenovo/Desktop/2017_Spring/CS6140/scripts")
# Setting up
###################################################################
###################################################################
# Set up work directory
#------------------------------------------------------------------
setwd("C:/Users/lenovo/Desktop/2017_Spring/CS6140/scripts")
# Read data
#------------------------------------------------------------------
X <- read.table('prostate.txt', header=TRUE, row.names=1, sep='\t')
head(X)
table(X$train)
# Select the training and the validation sets
#------------------------------------------------------------------
Xtrain <- X[X$train,-10]
Xvalid <- X[!X$train,-10]
# Exploration and modeling
###################################################################
###################################################################
# Data exploration
#------------------------------------------------------------------
summary(Xtrain)
pairs(Xtrain)
# Simple regression
#------------------------------------------------------------------
par(mfrow=c(2,2))
plot(lpsa ~ age, data=Xtrain, pch=16, col='red', ylim=c(0,5), main='lspa ~ age')
out <- lm(lpsa ~ age, data=Xtrain)
out
summary(out)
abline(lm(lpsa ~ age, data=Xtrain), col='grey')
# Additive multiple regression
#------------------------------------------------------------------
boxplot(lpsa ~ svi, data=Xtrain, col='orange', ylim=c(0,5))
out1 <- lm(lpsa ~ age+svi, data=Xtrain)
summary(out1)
plot(lpsa ~ age, data=Xtrain, pch=16, col='orange', ylim=c(0,5), main='lspa ~ age+svi')
lines(40:80, predict(out1, newdata=data.frame(age=40:80, svi=1)), lty=1)
lines(40:80, predict(out1, newdata=data.frame(age=40:80, svi=0)), lty=2)
# Multiple regression with interaction
#------------------------------------------------------------------
out2 <- lm(lpsa ~ age*svi, data=Xtrain)
summary(out2)
plot(lpsa ~ age, data=Xtrain, pch=16, col='green', ylim=c(0,5), main='lspa ~ age+svi+age*svi')
lines(40:80, predict(out2, newdata=data.frame(age=40:80, svi=1)), lty=1)
lines(40:80, predict(out2, newdata=data.frame(age=40:80, svi=0)), lty=2)
# Multiple regression with all predictors
#------------------------------------------------------------------
out3 <- lm(lpsa ~ ., data=Xtrain)
summary(out3)
names(out3)
# Checking of assumptions
###################################################################
###################################################################
# Residual plot
#------------------------------------------------------------------
plot(out2$fitted.values, out2$residuals)
abline(h=0)
plot(out2$residuals)
qqnorm(out2$residuals)
qqline(out2$residuals)
# Prediction on the validation set
###################################################################
###################################################################
par(mfrow=c(2,2))
# Prediction: compare models
#------------------------------------------------------------------
plot(Xvalid$lpsa, predict(out, newdata=Xvalid),
pch=16, col='red', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ age')
abline(a=0,b=1, col='grey')
plot(Xvalid$lpsa, predict(out1, newdata=Xvalid),
pch=16, col='orange', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ age+svi')
abline(a=0,b=1, col='grey')
plot(Xvalid$lpsa, predict(out2, newdata=Xvalid),
pch=16, col='green', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ age + svi + age*svi')
abline(a=0,b=1, col='grey')
plot(Xvalid$lpsa, predict(out3, newdata=Xvalid),
pch=16, col='blue', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ all')
abline(a=0,b=1, col='grey')
# Prediction: compare training and validation set
#------------------------------------------------------------------
par(mfrow=c(2,2))
plot(Xtrain$lpsa, predict(out, newdata=Xtrain),
pch=16, col='red', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='training, lspa ~ age')
abline(a=0,b=1, col='grey')
plot(Xvalid$lpsa, predict(out, newdata=Xvalid),
pch=16, col='red', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ age')
abline(a=0,b=1, col='grey')
plot(Xtrain$lpsa, predict(out3, newdata=Xtrain),
pch=16, col='blue', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='training, lspa ~ all')
abline(a=0,b=1, col='grey')
plot(Xvalid$lpsa, predict(out3, newdata=Xvalid),
pch=16, col='blue', xlim=c(1,6), ylim=c(1,6), xlab='observed', ylab='predicted',
main='validation, lspa ~ all')
abline(a=0,b=1, col='grey')
