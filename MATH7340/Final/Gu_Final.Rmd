---
title: "Math7340 Final"
author: "Chengbo Gu"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(multtest)
setwd("C:/Users/lenovo/Desktop/2017_Spring/MATH7340/Final")
```


### Problem 1. (10 points) MLE and boostrap for Poisson data
##### A random sample from the Poisson distribution $poisson(\lambda=e^{\theta})$ is provieded in the file "DataPois.txt"

##### (a) What if the sample size n? What is the sample mean $\bar{Y}$?
```{r}
y<-as.numeric(t(read.table(file="DataPois.txt", header=TRUE)))
length(y)
mean(y)
```

##### (b) Find the value of MLE $\hat{\theta}$ on this data set using numerical method.
```{r}
nloglik <- function(theta) -sum(log(dpois(y, lambda=exp(theta)))) #likelihood function
optim(par=1, nloglik)
```
##### (c) Test the null hypothesis that $\theta=1$ at level 0.05, using a bootstrap confidence interval. What is your conclusion? Can you find the p-value?
```{r warning=FALSE}
n <- length(y)
nboot <- 1000
boot.theta <- rep(NA, nboot)
for (i in 1:nboot) {
  y.star <- y[sample(1:n, replace=TRUE)]
  nloglik.boot <- function(theta) -sum(log(dpois(y.star, lambda=exp(theta)))) #likelihood function
  boot.theta[i] <- optim(par=1, nloglik.boot)$par
}
```
```{r}
quantile(boot.theta, c(0.025, 0.975))
```
```{r}
t.test(y, mu=exp(1))
```
```{r}
tstat <- (mean(y) - exp(1))/(sd(y)/sqrt(n))
2*pt(-abs(tstat), df=n-1)
```
The 95% boostrap confidence interval is (`r quantile(boot.theta, c(0.025, 0.975))[1]`, `r quantile(boot.theta, c(0.025, 0.975))[2]`) which doesn't include 1.

Thus, we reject the null hypothesis and conclude that $\theta\neq1$.

The p-value here is 2.695059e-09 which is extremely small. So we could reject the null hypothesis using p-value too.

### Problem 2. (20 points) ANOVA
##### We analyze the data set NCI60 data from the ISLR library.
##### (a) Delete the cancer types with only one or two cases (“K562A-repro”, etc.). Keep only the cancer types with more than 3 cases.
```{r}
library(ISLR)
library(lmtest)
ncidata <- NCI60$data
ncilabs <- as.factor(NCI60$labs)

filter <- as.vector(sapply(ncilabs, function(x) table(ncilabs)[x] > 2))
mydata <- ncidata[filter,]
mylabs <- ncilabs[filter]
```

##### (b) Analyze the expression values of the first gene in the data (first column). Does the first gene express differently in different types of cancers? If so, in which pairs of cancer types does the first gene express differently? (Use FDR adjustment.)
```{r}
anova( lm( mydata[,1] ~ mylabs ) )
```
Since the p-value for the types of cancers is 0.03928 which is less than 0.05, we reject the null hypothesis and conclude that the types of cancers do affect the gene express of first gene.

```{r}
pairwise.t.test(mydata[,1],  mylabs, p.adjust.method='fdr')
```

##### (c) Check the model assumptions for analysis in part (b). Is ANOVA analysis appropriate here?
```{r}
shapiro.test( residuals( lm( mydata[,1] ~ mylabs )))
bptest( lm( mydata[,1] ~ mylabs ), studentize = FALSE)
```

##### (d) Apply ANOVA analysis to each of the 6830 genes. At FDR level of 0.05, how many genes express differently among different types of cancer patients?
```{r}
#p.values <- apply(mydata,2, function(x) anova( lm( x ~ mylabs ))[["Pr(>F)"]][1])
p.values <- apply(mydata, 2, function(x) kruskal.test(x ~ mylabs)$p.value)
p.fdr <-p.adjust(p=p.values, method="fdr")
sum(p.fdr < 0.05)
```

### Problem 3. (10 points) Regression
##### We consider the regression analysis on the state.x77 data set. In the module 9, we regressed the life expectancy on three variables: the murder rates, percentage of high-school graduates and mean number of frost days.

##### (a) Make pairwise scatterplots for all variables in the data set. Which variables appears to be linearly correlated with the life expectancy based on the scatterplots?
```{r}
pairs(state.x77)
```

##### (b) Conduct a regression analysis different from the example analysis in module 9. We regress the life expectancy on three variables: the per capita income (Income), the illiteracy rate (Illiteracy) and mean number of frost days (Frost). What is your regression equation? In this regression analysis, which of the three variables affect the life expectancy significantly?
```{r}
data <- as.data.frame(state.x77[,c('Life Exp', 'Income', 'Illiteracy', 'Frost')])
names(data)<-c('Life.Exp', 'Income', 'Illiteracy', 'Frost')
lin.reg <- lm(Life.Exp~., data = data)
summary(lin.reg)
```
Life.Exp = 72.5260080 + 0.0001819Income - 1.5605544Illiteracy - 0.0060148Frost.
Illiteracy affects the life expectancy significantly.

##### (c) Find delete-one-cross-validated mean square errors $\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{(-i)})^2$ for this regression model.
```{r}
n <- dim(state.x77)[1]
err <- 0
for (i in 1:n) {
  data.tr <- data[-i,]
  lin.reg.tr <- lm(Life.Exp~., data = data.tr)
  err <- err + as.numeric(predict(lin.reg.tr, newdata=data[i,]) - data[i,1])^2
  #err
}
err <- err/n
err
```
### Problem 4. (60 points) Predicting B-cell differentiation with gene expression.
##### We analyze data for the B-cell patients in the ALL data set in the textbook.
##### (a) Select gene expression data for only the B-cell patients. The analysis in following parts will only use these gene expression data on the B-cell patients.
```{r}
library(ALL)
data(ALL)
Bcells <- ALL$BT %in% c('B', 'B1', 'B2', 'B3', 'B4')
ALLB <- exprs(ALL)[,Bcells]
```

##### (b) Select only those genes whose coefficient of variance (i.e., standard deviation divided by the mean) is greater than 0.2. How many genes are selected?
```{r}
library(genefilter)
func <- cv(0.2)
select <- genefilter(ALLB, filterfun(func))
data.filtered <- ALLB[select,]
label.filtered <- as.character(ALL$BT[Bcells])
dim(data.filtered)[1]
```
184 genes are selected.

##### (c) We wish to conduct clustering analysis to study natural groupings of the patients predicted by the gene expression profiles. For this analysis, we first need to reduce the number of genes studied. The filter in (b) is one such choice. Please comment on what filtering methods you would use to choose genes, other than the filter in (b). What would you consider as the best gene filter in this case.

Another criteria could be that the absolute expression levels of the genes were big enough.

##### (d) Conduct a hierarchical clustering analysis with filtered genes in (b). How do the clusters compare to the B-stages? How does do the clusters compare to the molecule biology types (in variable ALL$mol.biol)? Provide the confusion matrices of the comparisons, with 4 clusters.
```{r}
hc.complete <- hclust(dist(t(data.filtered), method="euclidian"), method="ward.D2")
plot(hc.complete, hang=-1, cex=0.38)
rect.hclust(hc.complete, k=4)
```

```{r}
groups <- cutree(hc.complete, k=4)
table(label.filtered, groups)
```
```{r}
MBT <- as.character(ALL$mol.biol[Bcells])
table(MBT, groups)
```

##### (e) Draw two heatmaps for the expression data in (d), one for each comparison. Using colorbars to show the comparison types (B-stages or molecule biology types). The clusters reflect which types better: B-stages or molecule biology types?
```{r}
library(gplots)
colnames(data.filtered) <- label.filtered
col.ord<-order(label.filtered) #Order REF/CR cases
dat1<-data.filtered[, col.ord] #Reorder columns, 6 REF cases first

```
```{r}
color.map <- function(B) {#color code for each type as in paper
  if (B=="B1") "yellow"
  else if(B=="B2") "lightblue"
  else if(B=="B3") "darkolivegreen4"
  else if(B=="B4") "darkorange"
  else "purple"
}
patientcolors<- unlist(lapply(colnames(dat1), color.map)) #apply color code
heatmap.2(dat1, col=greenred(75), margin=c(3, 12), scale="row", Rowv=FALSE, Colv=FALSE, key=FALSE, ColSideColors=patientcolors, trace="none", dendrogram="none", labRow=NA)
```

```{r}
col.ord<-order(MBT) #Order REF/CR cases
colnames(data.filtered) <- MBT
dat2<-data.filtered[, col.ord] #Reorder columns, 6 REF cases first
```
```{r}
color.map <- function(MB) {#color code for each type as in paper
  if (MB=="ALL1/AF4") "yellow"
  else if(MB=="BCR/ABL") "lightblue"
  else if(MB=="E2A/PBX1") "darkolivegreen4"
  else if(MB=="NEG") "darkorange"
  else "purple"
}
patientcolors<- unlist(lapply(colnames(dat2), color.map)) #apply color code
heatmap.2(dat2, col=greenred(75), margin=c(3, 12), scale="row", Rowv=FALSE, Colv=FALSE, key=FALSE, ColSideColors=patientcolors, trace="none", dendrogram="none", labRow=NA)
```

##### (f) We focus on predicting the B-cell differentiation in the following analysis. We merge the last two categories "B3" and "B4", so that we are studying 3 classes: "B1", "B2" and "B34". Use linear model (limma library) to select genes that expresses differently among these three classes at FDR of 0.05. How many genes are selected?
```{r warning=FALSE}
library(limma)
```
```{r}
Bcells.1 <- label.filtered %in% c('B1', 'B2', 'B3', 'B4')
label.filtered <- label.filtered[Bcells.1]
ALLB.1 <- ALLB[,Bcells.1]
mylabel <- as.factor(sapply(label.filtered, function(x) 
  {if((x == 'B3') || (x == 'B4')) 'B34' else x}))
design.ma <- model.matrix(~0 + mylabel)
colnames(design.ma) <- c("B1", "B2", "B34")
fit <- lmFit(ALLB.1, design.ma)
fit <- eBayes(fit)
cont.ma <- makeContrasts(B1-B2,B2-B34, levels=mylabel)
fit1 <- contrasts.fit(fit, cont.ma)
fit1 <- eBayes(fit1)
dim(topTable(fit1, number=Inf, p.value=0.05, adjust.method="fdr"))[1]
```

##### (g) Fit SVM and the classification tree on these selected genes in part (f), evaluate their performance with delete-one-cross-validated misclassification rate.
```{r}
```

##### (h) We select the genes passing both filters in (b) and (f). How many genes are selected? Redo part (g) on these genes passing both filters.
```{r}
data.filtered <- data.filtered[,Bcells.1]
label.filtered <- as.factor(sapply(label.filtered, function(x) 
  {if((x == 'B3') || (x == 'B4')) 'B34' else x}))
design.ma <- model.matrix(~0 + label.filtered)
colnames(design.ma) <- c("B1", "B2", "B34")
fit <- lmFit(data.filtered, design.ma)
fit <- eBayes(fit)
cont.ma <- makeContrasts(B1-B2,B2-B34, levels=label.filtered)
fit1 <- contrasts.fit(fit, cont.ma)
fit1 <- eBayes(fit1)
genes <- rownames(topTable(fit1, number=Inf, p.value=0.05, adjust.method="fdr"))
```

```{r}
data.final <- ALLB.1[genes,]
```

##### Which classifier you will consider best among the classifiers studied in part (g) and part (h)? Why?
